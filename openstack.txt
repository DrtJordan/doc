20150316
缺省启动的instance是不能持久的，需要单独分配block device才能持久和docker一样
通过 tenant(原来叫project)为单位来控制资源的使用(vlan,comopute,volume等等)

controller 节点 需要做HA
compute节点宕机前，可以通过 nova live-migration <uuid> c02.example.com迁移，宕机后，可以重启改节点上的instance

Neutron 网络目的是为 OpenStack 云更灵活地划分物理网络，在多租户环境下提供给每个租户独立的网络环境。支持三层，支持vlan
原来的 nova-network 就是一个二层的网桥网络 ,也支持dhcp/vlan
nova-network 只支持通过linux bridge通过计算节点的物理网卡连接外网
nova-network  支持multi-host方式，compute节点直接运行network ，floating ip也配置在compute节点上
neutron 暂时不支持multi-host
live-migration 也支持在非共享的存储上进行(kvm+libvirt支持QEMU 1.4 and libvirt 1.0.2)

在Openstack中，nova-network主要实现了以下一些功能，括号中是其实现时主要使用的工具：
NAT (iptables,  ip)
VLAN (ifconfig,  brctl)
DHCP Server (dnsmasq)
DNS Server (dnsmasq)
Firewall (iptables)

east-west指虚拟机内部的通信
south-north指虚拟机和外部的通信


创建instance 
CPU allocation ratio: 16:1
RAM allocation ratio: 1.5:1

当用户创建network， Neutron 会创建一个namespace，这个namespace通过OVS连接到虚拟机网络。OVS还负责namespace与虚拟机网络之间VLAN标签的修改。
ip netns list  
ip netns exec qrouter-2c3b419c-6220-4545-81c9-cd4f79a27d5f ip addr 
查看linux bridge 
brctl show 
bridge name     bridge id               STP enabled     interfaces
qbr8adb4037-f0          8000.26c788c34d4d       no              qvb8adb4037-f0
                                                        tap8adb4037-f0  (vm的网卡在vm内改名成eth0)
                                                        
查看ovs交换机的端口
ovs-vsctl show  
[root@dev177 log]# ovs-vsctl  show          
2fa66d20-bc85-49e0-b236-1edb86e3f783
    Bridge br-int
        fail_mode: secure
        Port patch-tun
            Interface patch-tun
                type: patch
                options: {peer=patch-int}
        Port "qvo8adb4037-f0"
            tag: 3
            Interface "qvo8adb4037-f0"
        Port br-int
            Interface br-int
                type: internal
    Bridge br-tun
        Port "gre-c0a8c1b0"
            Interface "gre-c0a8c1b0"
                type: gre
                options: {df_default="true", in_key=flow, local_ip="192.168.193.177", out_key=flow, remote_ip="192.168.193.176"}
        Port "gre-c0a8c1ae"
            Interface "gre-c0a8c1ae"
                type: gre
                options: {df_default="true", in_key=flow, local_ip="192.168.193.177", out_key=flow, remote_ip="192.168.193.174"}
        Port br-tun
            Interface br-tun
                type: internal
        Port "gre-c0a8c192"
            Interface "gre-c0a8c192"
                type: gre
                options: {df_default="true", in_key=flow, local_ip="192.168.193.177", out_key=flow, remote_ip="192.168.193.146"}
        Port patch-int
            Interface patch-int
                type: patch
           options: {peer=patch-tun}
                
查看veth的两端
ethtool -S   qvb8adb4037-f0
    peer_ifindex: 39  对应  qvo8adb4037-f0 连接到  ovs switch -> Bridge br-int
查看网卡类型
ethtool -i qvb16f6c29d-f4
不同网段的的网卡打的标记是不同的vlan 比如vm有两个网段的网卡，所以缺省不能ping通
 Bridge br-int
 	Port "qvo65ab2a9c-ae"
            tag: 2
            Interface "qvo65ab2a9c-ae"
        Port "qvo16f6c29d-f4"
            tag: 1
            Interface "qvo16f6c29d-f4"
查看 ovs data flow 
ovs-ofctl dump-flows br-tun
 ovs-ofctl show br-tun

虚拟机通过 tap 连接到 linux bridge,然后通过veth连接到 ovs
添加外部网络后，network的主机的router netspace 会有snat 
-A neutron-l3-agent-snat -s 10.0.0.0/24 -j SNAT --to-source 192.168.193.164
-A neutron-l3-agent-snat -s 10.0.1.0/24 -j SNAT --to-source 192.168.193.164
给某个vm添加浮动ip后
-A neutron-l3-agent-PREROUTING -d 192.168.193.165/32 -j DNAT --to-destination 10.0.1.3
-A neutron-l3-agent-float-snat -s 10.0.1.3/32 -j SNAT --to-source 192.168.193.165


VM  ->  tap8adb4037-f0  (virtual interface)  ->  qvb8adb4037-f0 (Linux bridge)  ->  qvo8adb4037-f0 (interface connected from Linux bridge to OVS bridge br-int)  ->  phy-br-eth2 (veth the other end)  ->  eth2 physical interface.  
可以对ovs设备创建一个mirror，然后就能用tcpdump监控
ip link add name snooper0 type dummy
ip link set dev snooper0 up
ovs-vsctl add-port br-int snooper0
ovs-vsctl -- set bridge br-int mirrors=@m -- --id=@snooper0 get Port snooper0 -- --id=@patch-tun get Port patch-tun -- --id=@m create Mirror name=mymirror select-dst-port=@patch-tun select-src-port=@patch-tun output-port=@snooper0
检查
ovs-vsctl list bridge br-int 
清除 mirror 
ovs-vsctl clear bridge br-int mirrors
ovs-vsctl del-port br-int snooper0
ip link delete dev snooper0
  
然后就可以 tcpdump -i snooper0

网络表结构 ports networks subnets routers
由于iptables不能用于OVS网桥
router其实只是一个额外的namespace
创建router时，Neutron会创建一个叫qrouter-的namespace。subnets通过OVS的br-int网桥上的网络接口接入router。网络接口被设置了正确的VLAN，从而可以连入它们对应的network。例子中，网络接口qr-0b7b0b40-f9的IP被设置为10.10.10.1，VLAN标签为1,它可以连接到“net1”。通过在namespace中设置系统参数net.ipv4.ip_forward为1，从而允许路由生效。
检查dhcp分配情况  /var/lib/neutron/dhcp/3e56ed15-2d35-45f8-98df-4aea15ee1437/host  
检查router是否打开了forward 
ip netns exec  qrouter-2c3b419c-6220-4545-81c9-cd4f79a27d5f sysctl net.ipv4.ip_forward 
控制节点
计算节点
网络节点
存储节点
先配置好主机名ip映射

controller安装

yum install -y  yum-plugin-priorities
yum install -y http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-5.noarch.rpm
yum install -y http://rdo.fedorapeople.org/openstack-juno/rdo-release-juno.rpm
yum upgrade
yum install -y  openstack-selinux

yum install -y  mariadb mariadb-server MySQL-python
修改mysql配置文件
[mysqld]
...
default-storage-engine = innodb
innodb_file_per_table
collation-server = utf8_general_ci
init-connect = 'SET NAMES utf8'
character-set-server = utf8


systemctl enable mariadb.service
systemctl start mariadb.service


yum install -y rabbitmq-server
systemctl enable rabbitmq-server.service
systemctl start rabbitmq-server.service
修改密码
rabbitmqctl change_password guest  rabbitmq

配置 Identity
CREATE DATABASE keystone;
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'localhost'   IDENTIFIED BY 'keystone';
GRANT ALL PRIVILEGES ON keystone.* TO 'keystone'@'%'   IDENTIFIED BY 'keystone';

openssl rand -hex 10
6ca2009ab8638bf86753

yum install openstack-keystone python-keystoneclient
修改 /etc/keystone/keystone.conf
[DEFAULT]
...
admin_token = 6ca2009ab8638bf86753
verbose = True
[database]
...
connection = mysql://keystone:keystone@192.168.193.145/keystone

[token]
...
provider = keystone.token.providers.uuid.Provider
driver = keystone.token.persistence.backends.sql.Token


keystone-manage pki_setup --keystone-user keystone --keystone-group keystone
chown -R keystone:keystone /var/log/keystone
chown -R keystone:keystone /etc/keystone/ssl
chmod -R o-rwx /etc/keystone/ssl

Populate the Identity service database:
su -s /bin/sh -c "keystone-manage db_sync" keystone

启动
systemctl enable openstack-keystone.service
systemctl start openstack-keystone.service

可以添加job清理 expired identity 
Identity service stores expired tokens in the database indefinitely
添加账户
export OS_SERVICE_TOKEN=6ca2009ab8638bf86753
export OS_SERVICE_ENDPOINT=http://dev145:35357/v2.0

keystone tenant-create --name admin --description "Admin Tenant"

keystone user-create --name admin --pass adminpwd --email wanzy@cicc.com.cn
keystone role-create --name admin
Add the admin role to the admin tenant and user:
keystone user-role-add --user admin --tenant admin --role admin
创建demo tenant 
keystone tenant-create --name demo --description "Demo Tenant"
keystone user-create --name demo --tenant demo --pass demopwd --email demo@cicc.com.cn

keystone tenant-create --name service --description "Service Tenant"

keystone service-create --name keystone --type identity   --description "OpenStack Identity"

keystone endpoint-create  --service-id $(keystone service-list | awk '/ identity / {print $2}')  --publicurl http://controller:5000/v2.0   --internalurl http://controller:5000/v2.0   --adminurl http://controller:35357/v2.0   --region regionOne


systemctl enable openstack-keystone.service
systemctl start openstack-keystone.service

验证id服务正常
keystone --os-tenant-name admin --os-username admin --os-password adminpwd   --os-auth-url http://controller:35357/v2.0 token-get

创建 glance服务
CREATE DATABASE glance;
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'localhost'  IDENTIFIED BY 'glancepwd';
GRANT ALL PRIVILEGES ON glance.* TO 'glance'@'%'   IDENTIFIED BY 'glancepwd';

admin-openrc.sh 
export OS_TENANT_NAME=admin
export OS_USERNAME=admin
export OS_PASSWORD=adminpwd
export OS_AUTH_URL=http://controller:35357/v2.0

demo-openrc.sh
export OS_TENANT_NAME=demo
export OS_USERNAME=demo
export OS_PASSWORD=demopwd
export OS_AUTH_URL=http://controller:35357/v2.0

keystone user-create --name glance --pass glancepwd
keystone user-role-add --user glance --tenant service --role admin
keystone service-create --name glance --type image  --description "OpenStack Image Service"

keystone endpoint-create   --service-id $(keystone service-list | awk '/ image / {print $2}')   --publicurl http://controller:9292   --internalurl http://controller:9292   --adminurl http://controller:9292   --region regionOne

 yum install openstack-glance python-glanceclient
 
 启动
 
 systemctl enable openstack-glance-api.service openstack-glance-registry.service
  systemctl restart openstack-glance-api.service openstack-glance-registry.service
 导入image 
 glance image-create --name "cirros-0.3.3-x86_64" --file cirros-0.3.3-x86_64-disk.img   --disk-format qcow2 --container-format bare --is-public True --progress
 
  glance image-create --name "centos7" --file centos-7.qcow2 --disk-format qcow2 --container-format bare --is-public True --progress
 
 验证
 glance image-list 
  
 安装计算节点(controller上)
 
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'localhost'  IDENTIFIED BY 'novapwd';
GRANT ALL PRIVILEGES ON nova.* TO 'nova'@'%'   IDENTIFIED BY 'novapwd';


keystone user-create --name nova --pass novapwd
keystone user-role-add --user nova --tenant service --role admin
keystone service-create --name nova --type compute   --description "OpenStack Compute"

keystone endpoint-create   --service-id $(keystone service-list | awk '/ compute / {print $2}')    --publicurl http://controller:8774/v2/%\(tenant_id\)s   --internalurl http://controller:8774/v2/%\(tenant_id\)s  --adminurl http://controller:8774/v2/%\(tenant_id\)s  --region regionOne

 yum install openstack-nova-api openstack-nova-cert openstack-nova-conductor   openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler   python-novaclient
 
 
 systemctl enable openstack-nova-api.service openstack-nova-cert.service   openstack-nova-consoleauth.service openstack-nova-scheduler.service  openstack-nova-conductor.service openstack-nova-novncproxy.service
 
 systemctl restart openstack-nova-api.service openstack-nova-cert.service   openstack-nova-consoleauth.service  openstack-nova-scheduler.service   openstack-nova-conductor.service openstack-nova-novncproxy.service
 
 安装计算节点(先把controller节点的 firewalld 停止)
 /etc/hosts
 192.168.193.145 dev145 controller
 公共安装
 yum install -y yum-plugin-priorities
 yum install -y http://dl.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-5.noarch.rpm
 yum install -y http://rdo.fedorapeople.org/openstack-juno/rdo-release-juno.rpm
 yum upgrade -y
 yum install -y openstack-selinux
 计算节点特定包
 yum install -y openstack-nova-compute sysfsutils
 
systemctl enable libvirtd.service openstack-nova-compute.service
systemctl restart libvirtd.service openstack-nova-compute.service 

验证

nova service-list
创建网络节点(controller上)

CREATE DATABASE neutron;
GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'localhost'    IDENTIFIED BY 'neutronpwd';
GRANT ALL PRIVILEGES ON neutron.* TO 'neutron'@'dev145'    IDENTIFIED BY 'neutronpwd';

keystone user-create --name neutron --pass neutronpwd
keystone user-role-add --user neutron --tenant service --role admin
keystone service-create --name neutron --type network    --description "OpenStack Networking"
keystone endpoint-create   --service-id $(keystone service-list | awk '/ network / {print $2}')   --publicurl http://controller:9696   --adminurl http://controller:9696   --internalurl http://controller:9696   --region regionOne

/bin/sh -c "neutron-db-manage --config-file /etc/neutron/neutron.conf   --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade juno" neutron
systemctl restart openstack-nova-api.service openstack-nova-scheduler.service   openstack-nova-conductor.service
systemctl enable neutron-server.service
systemctl restart neutron-server.service

  
安装网络节点

启动服务
systemctl enable openvswitch.service
systemctl restart openvswitch.service

systemctl enable neutron-openvswitch-agent.service neutron-l3-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-ovs-cleanup.service
systemctl restart neutron-openvswitch-agent.service neutron-l3-agent.service neutron-dhcp-agent.service neutron-metadata-agent.service neutron-ovs-cleanup.service

配置计算节点使用网络
systemctl enable openvswitch.service
systemctl restart openvswitch.service
systemctl restart openstack-nova-compute.service
systemctl enable neutron-openvswitch-agent.service
systemctl restart neutron-openvswitch-agent.service




创建网络在网络节点上 
ovs-vsctl add-br br-ex
vs-vsctl add-port br-ex INTERFACE_NAME
neutron net-create ext-net --router:external True   --provider:physical_network external --provider:network_type flat
neutron subnet-create ext-net --name ext-subnet    --allocation-pool start=192.168.193.164,end=192.168.193.172   --disable-dhcp --gateway 192.168.193.129 192.168.193.128/25


neutron net-create dev-net
neutron subnet-create dev-net --name dev-subnet --gateway 10.0.0.1 10.0.0.0/24

neutron net-create dev-net2
neutron subnet-create dev-net2 --name dev-subnet2 --gateway 10.0.1.1 10.0.1.0/24

neutron router-create dev-router
neutron router-interface-add dev-router dev-subnet
neutron router-interface-add dev-router dev-subnet2

neutron router-gateway-set dev-router ext-net
添加veth

创建instance
先生成 ssh_key 
ssh-keygen
nova keypair-add --pub-key ~/.ssh/id_rsa.pub dev-key
nova keypair-list
检查net id 
neutron net-list
nova boot --flavor m1.tiny --image cirros-0.3.3-x86_64 --nic net-id=3e56ed15-2d35-45f8-98df-4aea15ee1437   --security-group default --key-name dev-key dev-instance4

nova boot --flavor m1.small --image centos7    --nic net-id=3e56ed15-2d35-45f8-98df-4aea15ee1437   --security-group default --key-name dev-key dev-instance6

dnsmasq 的日志在 /var/log/message

检查服务状态
nova-manage service list
检查agent 状态
neutron agent-list

创建 centos image 
 
 
 qemu-img create -f qcow2 /vm/centos-7.qcow2 10G
 virt-install --virt-type kvm --name centos-7 --ram 2048 --cdrom=/vm/soft/CentOS-7.0-1406-x86_64-DVD.iso --disk path=/vm/centos-7.qcow2,size=10,format=qcow2 --network network=default --graphics vnc,listen=0.0.0.0 --noautoconsole --os-type=linux --os-variant=rhel7
 查看vnc 端口  vnc端口从5900开始 
 virsh vncdisplay centos-7
 :1 对应 5901
 
 错误，由于nova.conf配置本机ip地址出错，导致 vnc无法连接
 接着出现 exception: Invalid Token
 
 安装 dash board 
 访问地址 http://controller/dashboard 
 
 
 配置veth pair
ip netns add ns1
ip netns add ns2

brctl addbr br-test
brctl stp br-test
ip link set dev  br-test up
ip addr add 10.0.2.1/8 dev  br-test

#for ns1
ip link add tap1 type veth peer name br-tap1
brctl addif br-test br-tap1
ip link set tap1 netns ns1
ip netns exec ns1 ip link set dev tap1 up
ip link set dev br-tap1 up
ip addr add 10.0.2.2/8 dev tap1
#lo起来之后才能ping通本地的地址
ip link set lo up   
#for ns2
ip link add tap2 type veth peer name br-tap2
brctl addif br-test br-tap2
ip link set tap2 netns ns2
ip netns exec ns2 ip link set dev tap2 up
ip link set dev br-tap2 up
ip addr add 10.0.2.3/8 dev tap2
ip link set lo up

这样两个ns就能ping通了，firewalld关闭也能通
联通外网
ip netns exec ns1 route add default gw 10.0.2.1 tap1
#必须放到通外网的网卡上 或者不加网卡
iptables -t nat -I POSTROUTING -s 10.0.2.0/24 -o team0 -j MASQUERADE  

监控 ping 
tcpdump -e -i any -p icmp  and   host 192.168.193.176 

安装配置 openvswitch 
添加到 switch的网卡不能再有ip地址，但是可以把地址放到 交换机上面比如
ovs-vsctl add-br br0
ovs-vsctl add-port br0 eth0
ifconfig eth0 0.0.0.0 ; ifconfig br0 192.168.128.5