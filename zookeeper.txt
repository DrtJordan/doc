
所以这里也简单总结下，一旦发生会话超时，那么存储在ZK上的所有临时数据与注册的订阅者都会被移除，此时需要重新创建一个ZooKeeper客户端实例，需要自己编码做一些额外的处理。
如果只是网络闪断之类的，ZK客户端会自动重新连接
在实例化一个ZK客户端的时候，需要设置一个会话的超时时间。这里需要注意的一点是，客户端并不是可以随意设置这个会话超时时间，在ZK服务器端对会话超时时间是有限制的，主要是minSessionTimeout和maxSessionTimeout这两个参数设置的。（详细查看这个文章《ZooKeeper管理员指南》）Session超时时间限制，如果客户端设置的超时时间不在这个范围，那么会被强制设置为最大或最小时间。 默认的Session超时时间是在2 * tickTime ~ 20 * tickTime。


zookeeper


每秒能到10000左右的写操作(2-3客户端，写大概在50-60ms)，最多能到30000次，200ms就能重新选出leader 
数据都放在内存中，会有snapshot和trasnaction log (建议放在一个独立的磁盘上，提高性能)
客户可以连接到任意一个zk server进行读写(写操作会forward到leader上进行),如果server crash，客户端会自动连接到其他server上
zk的每次修改会有一个version，修改的时候先写入磁盘，然后在写入内存
zk保证 顺序执行原子操作，数据一致性 可靠性


zookeeper.connection.timeout 需要设置到30s左右，这样免得zk client和server来不及处理heartbeat断掉

zk shell bin/zkCli.sh -server ip:port
deploy的时候，保证所有机器clock一致
可以设置参数自动对snapshot和trsnaction log自动清理


目前zookeeper-monitor能做哪些事情，讲到这个，首先来看看哪些因素对zookeeper正常工作比较大的影响：
用于zookeeper写日志的目录要有足够大小，并且强烈建议在单独的磁盘（挂载点）上，这是影响ZK性能最大因素之一。
连接数。
注册的Watcher数。
ZNode是否可读，可写。
ZK事件通知的延时是否过大。

客户端如何正确处理CONNECTIONLOSS(连接断开) 和 SESSIONEXPIRED(Session 过期)两类连接异常
在ZooKeeper中，服务器和客户端之间维持的是一个长连接，在 SESSION_TIMEOUT 时间内，服务器会确定客户端是否正常连接(客户端会定时向服务器发送heart_beat),服务器重置下次SESSION_TIMEOUT时间。因此，在正常情况下，Session一直有效，并且zk集群所有机器上都保存这个Session信息。在出现问题情况下，客户端与服务器之间连接断了（客户端所连接的那台zk机器挂了，或是其它原因的网络闪断），这个时候客户端会主动在地址列表（初始化的时候传入构造方法的那个参数connectString）中选择新的地址进行连接。
好了，上面基本就是服务器与客户端之间维持长连接的过程了。在这个过程中，用户可能会看到两类异常CONNECTIONLOSS(连接断开) 和SESSIONEXPIRED(Session 过期)。
CONNECTIONLOSS发生在上面红色文字部分，应用在进行操作A时，发生了CONNECTIONLOSS，此时用户不需要关心我的会话是否可用，应用所要做的就是等待客户端帮我们自动连接上新的zk机器，一旦成功连接上新的zk机器后，确认刚刚的操作A是否执行成功了。
SESSIONEXPIRED发生在上面蓝色文字部分，这个通常是zk客户端与服务器的连接断了，试图连接上新的zk机器，这个过程如果耗时过长，超过 SESSION_TIMEOUT 后还没有成功连接上服务器，那么服务器认为这个session已经结束了（服务器无法确认是因为其它异常原因还是客户端主动结束会话），开始清除和这个会话有关的信息，包括这个会话创建的临时节点和注册的Watcher。在这之后，客户端重新连接上了服务器在，但是很不幸，服务器会告诉客户端SESSIONEXPIRED。此时客户端要做的事情就看应用的复杂情况了，总之，要重新实例zookeeper对象，重新操作所有临时数据（包括临时节点和注册Watcher）。

Watches通知是一次性的，必须重复注册.
发生CONNECTIONLOSS之后，只要在session_timeout之内再次连接上（即不发生SESSIONEXPIRED），那么这个连接注册的watches依然在。

Leader服务器会和每一个Follower/Observer服务器都建立TCP连接，同时为每个F/O都创建一个叫做LearnerHandler的实体。LearnerHandler主要负责Leader和F/O之间的网络通讯，包括数据同步，请求转发和Proposal提议的投票等。Leader服务器保存了所有F/O的LearnerHandler。


有参数设置初始化时候，多长时间内必须连接到leader上
可以通过只增长的节点创建的方式来实现分布式，比如创建节点 /cluster1/a- ，zookeepr会根据提交请求的先后顺序，自动
进行+1,这样客户创建成功后，在通过获取/cluster1下面所有的子节点信息来确认自己是不是最小的，如果是最小的，就认为自己活得锁
。当任意节点die(他获取的节点自动消失)或者join，所有watch的客户都会收到通知，客户可以确认自己是不是最小的

为了避免herd effect(所有客户都收到自己可能不感兴趣的消息),zookeeper做了一定的优化

创建session的时候，zookeeper会返回session id和password,如果该客户die，在session time out时间内用上次的session id和password则会认为是
同一session  ,session会自动发heartbeat信息 ,失败的时候session会自动重连

为了保证客户端连接的zookeeper是最新的同步数据，可以先做一个synetcat 操作，让该服务器和leader保持同步
也可以先做一个sync方法，然后再获取，保证是最新的数据
如果zookeeper crash重启之后，自动和leader同步，在没完成同步之前不接受新的请求
所有的写操作都是由leader同一完成和分发到follower上的，修改的时候先写到disk然后再写到内存，以免丢失，分发到follower上之后才返回给客户端

zookeeper自己的leader选举很快使用Zab协议(两阶段)，200ms就ok了。

3-5G内存就够了

支持事物的方式同时修改多个node

zookeeper里面的信息都是node,叫znode(最大的数据为1M),分为临时(session disconnect后自动删除)和持久的
没有递归删除功能，只能一个node一个node的删除


配置文件 /etc/hosts需要配置所有的相关机器名和ip

dataDir=/tmp/zookeeper
# the port at which the clients will connect
clientPort=2181
# disable the per-ip limit on the number of connections sinetcat e this is a non-production config
maxClientCnxns=0
tickTime=2000  
initLimit=5
synetcat Limit=2 
server.1=192.168.193.132:2182:3181
server.2=192.168.193.134:2182:3181
server.3=192.168.193.136:2182:3181  

同时需要在data目录的 myid里面填上自己的id好，比如 1

getChildren 加上watch之后收到一次通知后就失效了，需要再次watch
getChildren 返回的是按照字母顺序的，不一定是数字顺序，所以要遍历

启动 bin/zkServer.sh start 

只剩一个节点的时候，无法正常提供服务
剩两个节点的时候，能够正常提供服务

server.serverid=serverhost:leader_listent_port:quorum_port

顾名思义,serverid是本服务器的id,leader_listen_port是该服务器一旦成为leader之后需要监听的端口,用于接收来自follower的请求,quorum_port是集群中的每一个服务器在最开始选举leader时监听的端口,用于服务器互相之间通信选举leader.
每个节点需要在 dataDir下面生成一个配置文件 echo 1 > /tmp/zookeeper/myid  ，数字要唯一

以一个简单的例子来说明整个选举的过程.
假设有五台服务器组成的zookeeper集群,它们的id从1-5,同时它们都是最新启动的,也就是没有历史数据,在存放数据量这一点上,都是一样的.假设这些服务器依序启动,来看看会发生什么.
1) 服务器1启动,此时只有它一台服务器启动了,它发出去的报没有任何响应,所以它的选举状态一直是LOOKING状态
2) 服务器2启动,它与最开始启动的服务器1进行通信,互相交换自己的选举结果,由于两者都没有历史数据,所以id值较大的服务器2胜出,但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3),所以服务器1,2还是继续保持LOOKING状态.
3) 服务器3启动,根据前面的理论分析,服务器3成为服务器1,2,3中的老大,而与上面不同的是,此时有三台服务器选举了它,所以它成为了这次选举的leader.
4) 服务器4启动,根据前面的分析,理论上服务器4应该是服务器1,2,3,4中最大的,但是由于前面已经有半数以上的服务器选举了服务器3,所以它只能接收当小弟的命了.
5) 服务器5启动,同4一样,当小弟.

以上就是fastleader算法的简要分析,还有一些异常情况的处理,比如某台服务器宕机之后的处理,当leader宕机之后的处理等等,后面再谈.

  ZooKeeper 支持某些特定的四字命令字母与其的交互。它们大多是查询命令，用来获取 ZooKeeper 服务的当前状态及相关信息。用户在客户端可以通过 telnet 或 netcat  向 ZooKeeper 提交相应的命令

    1. 可以通过命令：echo stat|netcat  127.0.0.1 2181 来查看哪个节点被选择作为follower或者leader
    2. 使用echo ruok|netcat  127.0.0.1 2181 测试是否启动了该Server，若回复imok表示已经启动。
    3. echo dump| netcat  127.0.0.1 2181 ,列出未经处理的会话和临时节点。
    4. echo kill | netcat  127.0.0.1 2181 ,关掉server
    5. echo conf | netcat  127.0.0.1 2181 ,输出相关服务配置的详细信息。
    6. echo cons | netcat  127.0.0.1 2181 ,列出所有连接到服务器的客户端的完全的连接 / 会话的详细信息。
    7. echo envi |netcat  127.0.0.1 2181 ,输出关于服务环境的详细信息（区别于 conf 命令）。
    8. echo reqs | netcat  127.0.0.1 2181 ,列出未经处理的请求。
    9. echo wchs | netcat  127.0.0.1 2181 ,列出服务器 watch 的详细信息。
    10. echo wchc | netcat  127.0.0.1 2181 ,通过 session 列出服务器 watch 的详细信息，它的输出是一个与 watch 相关的会话的列表。
    11. echo wchp | netcat  127.0.0.1 2181 ,通过路径列出服务器 watch 的详细信息。它输出一个与 session 相关的路径。
    
    
  主从选举代码
  
  package com.cicc.oes.leaderelection;

import java.io.IOException;
import java.util.List;

import org.apache.log4j.Logger;
import org.apache.zookeeper.CreateMode;
import org.apache.zookeeper.KeeperException;
import org.apache.zookeeper.WatchedEvent;
import org.apache.zookeeper.Watcher;
import org.apache.zookeeper.Watcher.Event.EventType;
import org.apache.zookeeper.ZooDefs;
import org.apache.zookeeper.ZooKeeper;

public class LeaderElection implements Watcher, Runnable{

	private static Logger logger = Logger.getLogger(LeaderElection.class);
	
	private ZooKeeper zk;
	private String hostPortList;
	private int timeOut;
	private String rootNode;
	private String znode;
	private String nodePath;
	private LeaderElectedAction action;
	private volatile boolean isLeader = false;
    private Integer mutex;
	
	public LeaderElection(String hostPortList,int timeOut, String electionRootPath, String electionNodePrefix,LeaderElectedAction action){
		this.hostPortList = hostPortList;
		this.timeOut = timeOut;
		this.rootNode = electionRootPath;
		this.znode = electionNodePrefix;
		this.action = action;
		this.mutex = new Integer(-1);
	}
	
	@Override
	public void run() {
		
		try {
			zk = new ZooKeeper(hostPortList,timeOut,this);
			if(zk.exists(rootNode, false) == null){
				zk.create(rootNode, "for election".getBytes(), ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
			}
			nodePath = zk.create(znode+"_", null, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);
		  logger.info("path is "+nodePath);
		
		  while (true) {
	             synchronized (mutex) {
	             paticipateLeaderElection(); 
	             mutex.wait();
	             }
			 }
			 
		} catch (IOException e) {
			logger.error(e.getMessage(), e);
		} catch (KeeperException e) {
			logger.error(e.getMessage(), e);
		} catch (InterruptedException e) {
			logger.error(e.getMessage(), e);
		}
		
	}

	private void paticipateLeaderElection() throws KeeperException, InterruptedException {
		
		List<String> children = zk.getChildren(rootNode, false);
		int theSeq = getSequenceNum(nodePath);
		String precedingNodePath = nodePath;
		if(children != null && !children.isEmpty()){
			int min = getSequenceNum(children.get(0));
			int max = 0;

			for(int i = 0; i < children.size(); i++){
				int currentSeq = getSequenceNum(children.get(i));
				min = currentSeq < min ? currentSeq : min;
				if(currentSeq >= max && currentSeq < theSeq){
					precedingNodePath = children.get(i);
					max = currentSeq;
					logger.info("precedingNodePath is " +precedingNodePath);
				}
			}
			
			if(theSeq == min){
				
				if(!isLeader) //first time become to true
				{action.elected();
				//become the leader
				logger.info("elected as the leader: " + nodePath);
				}
				isLeader = true;
               }
			else
			{ //only monitor preceding node ,if it changed like deleted ,will check again ,to avoid herd effect
				zk.exists(rootNode+"/"+precedingNodePath, true) ;
			}
		}
		
	}

	private int getSequenceNum(String np) {
		String seq = np.substring(np.lastIndexOf("_") + 1);
		return Integer.parseInt(seq);
	}

	@Override
	public void process(WatchedEvent event) {
		logger.info("event received: " + event);
		 
		synchronized (mutex) {
			//if(EventType.NodeDeleted.equals(event.getType()))
	            mutex.notify();
	        }
	}

	public boolean isLeader() {
		return isLeader;
	}

}

package com.cicc.oes.leaderelection;

public interface LeaderElectedAction {

	public void elected();
}


package com.cicc.oes.leaderelection;

public class TestZK implements  LeaderElectedAction{

	public static void main(String[] args) {
		// TODO Auto-generated method stub
		TestZK zk=new TestZK();
    	LeaderElection lc=new LeaderElection("192.168.193.135:2181,192.168.193.134:2181",5000, "/PTAMsgRouter", "/PTAMsgRouter/Router",zk);
    	new Thread(lc).start(); //或者直接 lc.start();则当前线程阻塞，直到有消息进来被zk的socket线程唤醒
	}
	public void elected()
	{
    System.out.println("I'm master now");		
	}
}
