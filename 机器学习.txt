机器学习
 
机器学习的输入是数据（Data），学到的结果叫模型（Model）  从数据中学得模型这个过程通过执行某个学习算法（Learning Algorithm）来完成。
机器学到的模型是一个映射。难以用规则解决的问题，可以尝试用机器学习来解决 永远不要跟机器学习专家说：“加条规则呗”
训练数据的属性称为 特征(feature)

根据用户的人口统计学推荐(比如性别，年龄，地域等）
基于内容的推荐(比如物品本身的标签)，协同过滤(比如物品和用户的关联关系）
基于关联规则的推荐(比如那些物品经常一起购买，或者用户购买一些物品后通常会再购买那些其他物品等)
基于模型的推荐(通过算法对数据做训练，找出模型)
基于模型的协同过滤不需要机器理解物品的属性，纯粹是数学计算

混合推荐
加权混合  切换混合 分区混合 分层混合(采用多种推荐机制，一个推荐的结果作为另一个的输入)
通常要对数据进行降噪归一处理
电商通常采用 itemCF ,新闻类通常采用 userCF(物品比人多) 通常两个算法一起用 
算法要考虑推荐的多样性(覆盖率)和精度
聚类分析通常用于文档分类，有基于概率分布和基于距离两种

拉格朗日乘子法 是一种寻找多元函数在一组约束下的极值的方法

监督学习：给定的采样数据已经包含结果   决策树/神经网络/回归/贝叶斯/KNN          监督学习又大致分成两类：分类（Classification）和回归（Regression） 价格作为标注就是一个连续值，属于回归问题。
非监督学习：给定的采样数据没有结果，需要通过模型来判断分类  聚类模型  （k-means)
半监督：少量标注样本和大量未标注样本
强化学习：输入数据作为对模型的反馈


 PLA，全称 Perceptron Learning Algorithm。其中 Perceptron 译作感知机，它是人工神经网络中最基础的两层神经网络模型
训练数据是线性可分的 (Liner Seperable) ，是 PLA 能够学习的必要条件。

回归问题，人的经验很重要，要通过人分析数据，画图先确定曲线的形状。
逻辑回归 是分类函数，取值通过 sigmoid 函数变成(0,1)之间的值 ，可以认为是某件事情发生的可能性
决策树的关键步骤是分裂属性，就是在某个节点处按照某一特征属性的不同进行分支，要尽可能的纯，就是相同的分到同一个分支下面。
属性选择的核心思想就是以信息增益度量属性最大的进行分裂，ID3算法在每次分裂的时，需要计算每个属性的增益度，选择最大的
ID3的缺点是偏向选择多值属性，比如，存在唯一标识属性ID，就一定会选择ID作为分裂属性。所以C4.5 使用增益率来优化
实际过程中，会进行剪枝，就是进行降噪处理

贝叶斯定理：
　P(A|B) = P(B|A) P(A) / P(B)
假设某个体有n项特征（Feature），分别为F1、F2、...、Fn。现有m个类别（Category），分别为C1、C2、...、Cm。贝叶斯分类器就是计算出概率最大的那个分类，也就是求下面这个算式的最大值：
　P(C|F1F2...Fn) 
　　= P(F1F2...Fn|C)P(C) / P(F1F2...Fn)
朴素贝叶斯分类器则是更进一步，假设所有特征都彼此独立 在现实中不太可能成立，但是它可以大大简化计算

SVM是让应用数学家真正得到应用的一种算法
  线圈出来的点就是所谓的支持向量(support vector)  Maximum Marginal，是SVM的一个理论基础之一。选择使得间隙最大的函数作为分割平面
  让空间从原本的线性空间变成一个更高维的空间，在这个高维的线性空间下，再用一个超平面进行划分
  用这个函数可以将平面中的点映射到一个三维空间（z1,z2,z3)  对映射后的坐标加以旋转之后就可以得到一个线性可分的点集了
  对于所有的两个物体，我们可以通过增加维度来让他们最终有所区别，比如说两本书，从(颜色，内容)两个维度来说，可能是一样的，我们可以加上 作者 这个维度，是在不行我们还可以加入页码，可以加入 拥有者，可以加入 购买地点，可以加入 笔记内容等等。当维度增加到无限维的时候，一定可以让任意的两个物体可分了
  
  聚类属于无监督学习，以往的回归、朴素贝叶斯、SVM等都是有类别标签y的，也就是说样例中已经给出了样例的分类。
  而聚类的样本中却没有给定y，只有特征x，比如假设宇宙中的星星可以表示成三维空间中的点集 。聚类的目的是找到每个样本x潜在的类别y，并将同类别y的样本x放在一起
  K-means算法是将样本聚类成k个簇（cluster）
  K-Means的算法如下：
1.	随机在图中取K（这里K=2）个种子点。
2.	然后对图中的所有点求到这K个种子点的距离，假如点Pi离种子点Si最近，那么Pi属于Si点群。（上图中，我们可以看到A,B属于上面的种子点，C,D,E属于下面中部的种子点）
3.	接下来，我们要移动种子点到属于他的“点群”的中心。（见图上的第三步）
4.	然后重复第2）和第3）步，直到，种子点没有移动（我们可以看到图中的第四步上面的种子点聚合了A,B,C，下面的种子点聚合了D，E）。

K-Means主要有两个最重大的缺陷――都和初始值有关：
?	 K 是事先给定的，这个 K 值的选定是非常难以估计的。很多时候，事先并不知道给定的数据集应该分成多少个类别才最合适。（ ISODATA 算法通过类的自动合并和分裂，得到较为合理的类型数目 K）
?	K-Means算法需要用初始随机种子点来搞，这个随机种子点太重要，不同的随机种子点会有得到完全不同的结果。（K-Means++算法可以用来解决这个问题，其可以有效地选择初始点）
K-Means++算法步骤：
1.	先从我们的数据库随机挑个随机点当“种子点”。
2.	对于每个点，我们都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。
3.	然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其<=0，此时的点就是下一个“种子点”。
4.	重复第（2）和第（3）步直到所有的K个种子点都被选出来。
5.	进行K-Means算法。

KNN算法和K-Means算法不同的是，K-Means算法用来聚类，用来判断哪些东西是一个比较相近的类型，而KNN算法是用来做归类的，也就是说，有一个样本空间里的样本分成很几个类型，然后，给定一个待分类的数据，通过计算接近自己最近的K个样本来判断这个待分类数据属于哪个分类。你可以简单的理解为由那离自己最近的K个点来投票决定待分类数据归为哪一类。

 

用户u1喜欢的电影是A，B，C
用户u2喜欢的电影是A, C, E, F
用户u3喜欢的电影是B，D
我们需要解决的问题是：决定对u1是不是应该推荐F这部电影
基于内容的做法：要分析F的特征和u1所喜欢的A、B、C的特征，需要知道的信息是A（战争片），B（战争片），C（剧情片），如果F（战争片），那么F很大程度上可以推荐给u1，这是基于内容的做法，你需要对item进行特征建立和建模。
协同过滤的办法：那么你完全可以忽略item的建模，因为这种办法的决策是依赖user和item之间的关系，也就是这里的用户和电影之间的关系。我们不再需要知道ABCF哪些是战争片，哪些是剧情片，我们只需要知道用户u1和u2按照item向量表示，他们的相似度比较高，那么我们可以把u2所喜欢的F这部影片推荐给u1。

用户u1喜欢的电影是A，B，C
用户u2喜欢的电影是A, C, E, F
用户u3喜欢的电影是B，D
我们需要解决的问题是：决定对u1是不是应该推荐F这部电影
基于内容的做法：要分析F的特征和u1所喜欢的A、B、C的特征，需要知道的信息是A（战争片），B（战争片），C（剧情片），如果F（战争片），那么F很大程度上可以推荐给u1，这是基于内容的做法，你需要对item进行特征建立和建模。
协同过滤的办法：那么你完全可以忽略item的建模，因为这种办法的决策是依赖user和item之间的关系，也就是这里的用户和电影之间的关系。我们不再需要知道ABCF哪些是战争片，哪些是剧情片，我们只需要知道用户u1和u2按照item向量表示，他们的相似度比较高，那么我们可以把u2所喜欢的F这部影片推荐给u1。
 
 赖杰明  13825261947  laijm@dcits.com
 