
机器学习

机箱
直接选跟 Nvidia DevBox 同款的机箱，即 Corsair Carbide Air 540，工作站需要足够的空间，材质稳定，大多深度学习工作站都选取此款机箱。
硬性要求：
X99平台
四个 PCI-E Gen3 x16 接口
众所周知单机多卡进行训练时，总线带宽是瓶颈，所以主板有越多的PCI-e 3.0 接口越好，可以尽可能把显卡的性能发挥出来。计划上四块 GTX 1080，那么至少需要四路 PCI-e 3.0 的 x16 接口。
Nvidia DevBox 用的是 Asus X99-E WS 
华硕 X99-E WS/USB 3.1 是 Asus X99-E WS 的新版
众所周知单机多卡进行训练时，总线带宽是瓶颈，所以 CPU 的 PCI-e lane 越多越好，一般消费级的CPU，PCI-e总线根数是 16, 28 或 40，最大就是40，再大就需要上服务器CPU或者双路CPU了。选40， 这个条件下，有两款 CPU 入围，I7-5930K和 I7-5960X, 5860X太贵了，一般机器学习训练中CPU不是瓶颈，所以选 5930K就可以了，这也是 Nvidia 官方推出的 DevBox 工作站所使用的CPU。
再大就没必要了，李沐的这篇博客GPU集群折腾手记 末尾有说到过，单机4卡的机器，64G内存绰绰有余了。
一般 Nvidia 的旗舰显卡，功耗都是压着 300W的线的，四卡就 1200W了，加上主板，CPU等耗电，选一个 1600W的电源吧。

Nvidia DevBox 用的是 EVGA 1600W 80+ Gold 120-G2-1600-X1 ，那我也用它吧。



如果要做事，想赶快入门，速度出活，请先死记住：
 深度学习=多层的神经网络
如果要写论文，要作报告，要闲聊，请坚持一个原则：
深度学习绝不仅仅是多层的神经网络。
神经网络最重要的用途是分类
 
机器学习的输入是数据（Data），学到的结果叫模型（Model）  从数据中学得模型这个过程通过执行某个学习算法（Learning Algorithm）来完成。
机器学到的模型是一个映射。难以用规则解决的问题，可以尝试用机器学习来解决 永远不要跟机器学习专家说：“加条规则呗”
训练数据的属性称为 特征(feature)

根据用户的人口统计学推荐(比如性别，年龄，地域等）
基于内容的推荐(比如物品本身的标签)，协同过滤(比如物品和用户的关联关系）
基于关联规则的推荐(比如那些物品经常一起购买，或者用户购买一些物品后通常会再购买那些其他物品等)
基于模型的推荐(通过算法对数据做训练，找出模型)
基于模型的协同过滤不需要机器理解物品的属性，纯粹是数学计算

混合推荐
加权混合  切换混合 分区混合 分层混合(采用多种推荐机制，一个推荐的结果作为另一个的输入)
通常要对数据进行降噪归一处理
电商通常采用 itemCF ,新闻类通常采用 userCF(物品比人多) 通常两个算法一起用 
算法要考虑推荐的多样性(覆盖率)和精度
聚类分析通常用于文档分类，有基于概率分布和基于距离两种

拉格朗日乘子法 是一种寻找多元函数在一组约束下的极值的方法

监督学习：给定的采样数据已经包含结果   决策树/神经网络/回归/贝叶斯/KNN          监督学习又大致分成两类：分类（Classification）和回归（Regression） 价格作为标注就是一个连续值，属于回归问题。
非监督学习：给定的采样数据没有结果，需要通过模型来判断分类  聚类模型  （k-means)
半监督：少量标注样本和大量未标注样本
强化学习：输入数据作为对模型的反馈

分类：LR，SVM，朴素贝叶斯，决策树，HMM NN
聚类：k-means，Dirichlet Process，Minhash，Canopy，Spectral
回归：Linear Regression
特征选择：SVD，PCA，ICA
关联规则：FP growth
推荐算法：ItemCF
时间序列：exponential smoothing


 PLA，全称 Perceptron Learning Algorithm。其中 Perceptron 译作感知机，它是人工神经网络中最基础的两层神经网络模型
训练数据是线性可分的 (Liner Seperable) ，是 PLA 能够学习的必要条件。

回归问题，人的经验很重要，要通过人分析数据，画图先确定曲线的形状。
逻辑回归 是分类函数，取值通过 sigmoid 函数变成(0,1)之间的值 ，可以认为是某件事情发生的可能性
决策树的关键步骤是分裂属性，就是在某个节点处按照某一特征属性的不同进行分支，要尽可能的纯，就是相同的分到同一个分支下面。
属性选择的核心思想就是以信息增益度量属性最大的进行分裂，ID3算法在每次分裂的时，需要计算每个属性的增益度，选择最大的
ID3的缺点是偏向选择多值属性，比如，存在唯一标识属性ID，就一定会选择ID作为分裂属性。所以C4.5 使用增益率来优化
实际过程中，会进行剪枝，就是进行降噪处理 
使用熵来表示不定性的度量 熵越大，随机变量的不确定越大
信息增益是针对某个特征，看系统有它和没有它信息量各是多少，两者的差就是这个特征给系统带来的信息量，也就是信息增益
信息增益大表明信息增多，则不确定性就越小。
信息增益的思想来源于信息论的香农定理，定义为离散随机事件出现的概率，越有序，信息熵就越小 ，反正越高
CART=classification and regresssion tree  是一种二分地柜分割技术，先把当年样本分为两个子样本。然后计算Gini指标，以最小Gini作为分裂属性
随机森林 随机森林由多个决策树注册，因为这些决策树的形成采用了随机的方法，所以叫随机森林，最后取所有决策树中分类结果最多的那个为最终结果

神经网络的用途就是分类，一条直线把平面一分为二，一个平面把三维空间一份为二，一个n-1维超平面把N维空间一分为二，两边各属于不同的类
深度学习=多个隐层的神经网络  主要用于解决线性不可分的问题




贝叶斯定理：
　P(A|B) = P(B|A) P(A) / P(B)
假设某个体有n项特征（Feature），分别为F1、F2、...、Fn。现有m个类别（Category），分别为C1、C2、...、Cm。贝叶斯分类器就是计算出概率最大的那个分类，也就是求下面这个算式的最大值：
　P(C|F1F2...Fn) 
　　= P(F1F2...Fn|C)P(C) / P(F1F2...Fn)
朴素贝叶斯分类器则是更进一步，假设所有特征都彼此独立 在现实中不太可能成立，但是它可以大大简化计算

SVM是让应用数学家真正得到应用的一种算法
  线圈出来的点就是所谓的支持向量(support vector)  Maximum Marginal，是SVM的一个理论基础之一。选择使得间隙最大的函数作为分割平面
  让空间从原本的线性空间变成一个更高维的空间，在这个高维的线性空间下，再用一个超平面进行划分
  用这个函数可以将平面中的点映射到一个三维空间（z1,z2,z3)  对映射后的坐标加以旋转之后就可以得到一个线性可分的点集了
  对于所有的两个物体，我们可以通过增加维度来让他们最终有所区别，比如说两本书，从(颜色，内容)两个维度来说，可能是一样的，我们可以加上 作者 这个维度，是在不行我们还可以加入页码，可以加入 拥有者，可以加入 购买地点，可以加入 笔记内容等等。当维度增加到无限维的时候，一定可以让任意的两个物体可分了
  
  聚类属于无监督学习，以往的回归、朴素贝叶斯、SVM等都是有类别标签y的，也就是说样例中已经给出了样例的分类。
  而聚类的样本中却没有给定y，只有特征x，比如假设宇宙中的星星可以表示成三维空间中的点集 。聚类的目的是找到每个样本x潜在的类别y，并将同类别y的样本x放在一起
  K-means算法是将样本聚类成k个簇（cluster）
  K-Means的算法如下：
1.	随机在图中取K（这里K=2）个种子点。
2.	然后对图中的所有点求到这K个种子点的距离，假如点Pi离种子点Si最近，那么Pi属于Si点群。（上图中，我们可以看到A,B属于上面的种子点，C,D,E属于下面中部的种子点）
3.	接下来，我们要移动种子点到属于他的“点群”的中心。（见图上的第三步）
4.	然后重复第2）和第3）步，直到，种子点没有移动（我们可以看到图中的第四步上面的种子点聚合了A,B,C，下面的种子点聚合了D，E）。

K-Means主要有两个最重大的缺陷――都和初始值有关：
?	 K 是事先给定的，这个 K 值的选定是非常难以估计的。很多时候，事先并不知道给定的数据集应该分成多少个类别才最合适。（ ISODATA 算法通过类的自动合并和分裂，得到较为合理的类型数目 K）
?	K-Means算法需要用初始随机种子点来搞，这个随机种子点太重要，不同的随机种子点会有得到完全不同的结果。（K-Means++算法可以用来解决这个问题，其可以有效地选择初始点）
K-Means++算法步骤：
1.	先从我们的数据库随机挑个随机点当“种子点”。
2.	对于每个点，我们都计算其和最近的一个“种子点”的距离D(x)并保存在一个数组里，然后把这些距离加起来得到Sum(D(x))。
3.	然后，再取一个随机值，用权重的方式来取计算下一个“种子点”。这个算法的实现是，先取一个能落在Sum(D(x))中的随机值Random，然后用Random -= D(x)，直到其<=0，此时的点就是下一个“种子点”。
4.	重复第（2）和第（3）步直到所有的K个种子点都被选出来。
5.	进行K-Means算法。

KNN算法和K-Means算法不同的是，K-Means算法用来聚类，用来判断哪些东西是一个比较相近的类型，而KNN算法是用来做归类的，也就是说，有一个样本空间里的样本分成很几个类型，然后，给定一个待分类的数据，通过计算接近自己最近的K个样本来判断这个待分类数据属于哪个分类。你可以简单的理解为由那离自己最近的K个点来投票决定待分类数据归为哪一类。

 

用户u1喜欢的电影是A，B，C
用户u2喜欢的电影是A, C, E, F
用户u3喜欢的电影是B，D
我们需要解决的问题是：决定对u1是不是应该推荐F这部电影
基于内容的做法：要分析F的特征和u1所喜欢的A、B、C的特征，需要知道的信息是A（战争片），B（战争片），C（剧情片），如果F（战争片），那么F很大程度上可以推荐给u1，这是基于内容的做法，你需要对item进行特征建立和建模。
协同过滤的办法：那么你完全可以忽略item的建模，因为这种办法的决策是依赖user和item之间的关系，也就是这里的用户和电影之间的关系。我们不再需要知道ABCF哪些是战争片，哪些是剧情片，我们只需要知道用户u1和u2按照item向量表示，他们的相似度比较高，那么我们可以把u2所喜欢的F这部影片推荐给u1。

用户u1喜欢的电影是A，B，C
用户u2喜欢的电影是A, C, E, F
用户u3喜欢的电影是B，D
我们需要解决的问题是：决定对u1是不是应该推荐F这部电影
基于内容的做法：要分析F的特征和u1所喜欢的A、B、C的特征，需要知道的信息是A（战争片），B（战争片），C（剧情片），如果F（战争片），那么F很大程度上可以推荐给u1，这是基于内容的做法，你需要对item进行特征建立和建模。
协同过滤的办法：那么你完全可以忽略item的建模，因为这种办法的决策是依赖user和item之间的关系，也就是这里的用户和电影之间的关系。我们不再需要知道ABCF哪些是战争片，哪些是剧情片，我们只需要知道用户u1和u2按照item向量表示，他们的相似度比较高，那么我们可以把u2所喜欢的F这部影片推荐给u1。
 
 赖杰明  13825261947  laijm@dcits.com
 