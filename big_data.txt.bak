国寿68个节点hdfs,交易数据全集中,用hbase和hbase反向索引表，用hbase+spark sql 
以json的形式存标签

kaggle 竞赛
标签属性：
姓名：韩宝龙 年龄：28 性别：男 所在地：北京 车辆类型：迈巴赫、奥迪、路虎 流失记录：有1次 历史三者保额：100万 …… （一般多个页面才能展示）
基础标签：
VIP5、富二代、旅游/户外爱好者、处女座、服务敏感、超速行驶、风险规避……
预测标签：
高流失风险、三者保额可提升、互联网偏好、早联系早得
土豪/商务人士/飞行达人/旅游达人/人脉广/违章 驾驶

深度学习是解决线性不可分的问题 
在限定领域机器人可用 1 问题空间变小2人对机器的期望会降低

GBDT gradient boosting decision tree 

在用户画像建模方面，我们把标签建模分为四层：
第一层是事实类标签，譬如用户购物了什么品类；
第二层是机器学习模型的预测标签，譬如当下需求、潜在需求等等；
第三层是营销模型类标签，譬如用户价值、活跃度和忠诚度等等；
第四层是业务类的标签，譬如高奢人群、有房一族等等，它是由底层的标签组合生成的，通常由业务人员定义。


现有用户->我的用户是谁？为什么买我的产品？他们有什么偏好？哪些用户价值更高
潜在客户->潜在用户在哪里？他们喜欢什么?哪些渠道能找到他们?获客成本是多少
目的是对内：指导产品研发(造什么客户买什么变成用户要什么造什么)，完善产品运营
对外精准销售
每一次用户行为 重点是用户什么时候在什么地方做了什么事情
用户近期发生的行为权重占比更大，过往行为权重降低
标签权重=时间衰减（何时）×网址权重（何地）×行为权重（做什么）
If you can’t measure it,  you can’t improve it

● 面向什么用户和场景
● 解决什么问题 / 带来什么价值
● 问题的分析思路是什么
● 需要用到什么样的指标
● 这些指标该怎么组合展现

将复杂的商业问题转化为数学模型，并利用编程能力进行分析，预测和评估，再转化为合适的Business Plan，执行。

report 告诉你过去发生什么
BI告诉你现在发生什么
modeling 告诉你将来发生什么

数据和特征决定了机器学习的上限，模型和参数决定了逼近这个上限的程度。

理想中的算法工程师：提出假设->收集数据->训练模型->解释结果。
实际中的算法工程师：提出假设->收集数据->预处理->预处理->训练模型->调试->调试->重新收集数据->预处理->收集更多数据->调试->调试->调试->...->放弃。


CLTV（Customer Lifetime Value用户生命价值），CLT（用户生命周期），Retention Rate （留存率），CAC （Cost of Acquisition 获客成本），COR（Cost of Retention 留存成本），CPI （Cost per Install 安装成本）

RFM分析（Recency，Frequency，Monetary）

玩耍本来是小孩的内部动机，不需要外部的刺激和强化就能一直持续下去，但犹太老头通过外部动机（给钱）使得内部动机的作用弱化，行为的持续依赖于外部动机。而后他撤销外部动机，小孩玩耍的行为就消失了~对于用户也是这样的，用户大致可以分成狂热者、推荐者、轻度使用者、跟随者和冷漠者这么几个级别，显然应该对他们区别对待。对于那些内部动机很强的用户，只要不施加过多外部动机影响导致破坏他们的内部动机就可以了；而对于不怎么感兴趣的用户，可能更多地采用一些鲜明的实质性刺激吧；对于中间一部分摇摆不定的用户，比较经济实惠而且能长期维持的方式，还是尽可能培养他们的内部动机，把他们转变成狂热者，而不是为了某种实质奖励的势利型用户，不然很容易流失。

 


通过数据建模，企业可以有效地为能覆盖到的用户打上标签，之后结合渠道信息和商品信息，企业可根据需求定向地选择数据挖掘的方法输出结果，在营销决策中，可能得到的结论例如“具有标签a的人集中购买了商品A”、“购买商品B的用户同样会对商品A感兴趣”、“商品A的购买人群主要集中于渠道c”等等，这些信息将直接指导企业完成营销决策。在这个过程中常用的算法包括聚类和关联规则等，本文不深入展开，这些算法的核心逻辑可以认为是利用现有事实对未来进行预测的过程。



三层用户标签体系中，共存在50多个一级标签，1000多个二级标签和近30万的三级标签;其中一级标签是大类标签，类似于新闻客户端中常见的频道(如右半部分的财经、互联网等)，二级标签是从属于一级标签的细分(如右半部分中财经下的股票，互联网下的互联网安全等)，三级标签是对二级标签的进一步细分，能对应到一级标签下的实体对象(如财经下某一支具体的股票，互联网下的某一家具体公司等)。

数据仓库（Data Warehouse）是一个面向主题的（Subject Oriented）、集成的（Integrated）、相对稳定的（Non-Volatile）、反应历史变化（Time Variant）的数据集合，用于支持管理决策。
当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的 "?层次 " 区域，这些被分解的表都连接到主维度表而不是事实表。如图 2，将地域维表又分解为国家，省份，城市等维表。它的优点是 :?通过最大限度地减少数据存储量以及联合较小的维表来改善查询性能。雪花型结构去除了数据冗余。
OLAP的基本多维分析操作有钻取（roll up和drill down）、切片（slice）和切块（dice）、以及旋转（pivot）、drill across、drill through等。
・钻取是改变维的层次，变换分析的粒度。它包括向上钻取（roll up）和向下钻取（drill down）。roll up是在某一维上将低层次的细节数据概括到高层次的汇总数据，或者减少维数；而drill down则相反，它从汇总数据深入到细节数据进行观察或增加新维。
・切片和切块是在一部分维上选定值后，关心度量数据在剩余维上的分布。如果剩余的维只有两个，则是切片；如果有三个，则是切块。
・旋转是变换维的方向，即在表格中重新安排维的放置（例如行列互换）。
ROLAP表示基于关系数据库的OLAP实现（Relational OLAP）。以关系数据库为核心,以关系型结构进行多维数据的表示和存储。ROLAP将多维数据库的多维结构划分为两类表:一类是事实表,用来存储数据和维关键字;另一类是维表,即对每个维至少使用一个表来存放维的层次、成员类别等维的描述信息。维表和事实表通过主关键字和外关键字联系在一起,形成了'星型模式'。对于层次复杂的维,为避免冗余数据占用过大的存储空间,可以使用多个表来描述,这种星型模式的扩展称为'雪花模式'。特点是将细节数据保留在关系型数据库的事实表中，聚合后的数据也保存在关系型的数据库中。这种方式查询效率最低，不推荐使用。
MOLAP表示基于多维数据组织的OLAP实现（M?ultidimensional OLAP）。以多维数据组织方式为核心,也就是说,MOLAP使用多维数组存储数据。多维数据在存储中将形成'立方块（Cube）'的结构,在MOLAP中对'立方块'的'旋转'、'切块'、'切片'是产生多维数据报表的主要技术。特点是将细节数据和聚合后的数据均保存在cube中，所以以空间换效率，查询时效率高，但生成cube时需要大量的时间和空间。


大数据征信：在个人征信领域，目前是金融行业面临的最大问题。基于用户在互联网上的消费行为、社交行为、搜索行为等产生的海量数据，其价值并未被充分挖掘，个人征信在大数据的采集和信息挖掘上面仍有很大的想象空间。
大数据风控：大数据风控目前应该是前沿技术在金融领域的最成熟应用，相对于智能投顾、区块链等还在初期的金融科技应用，大数据风控目前已经在业界逐步普及。目前，美国基本上都用三大征信局的信息，最传统的评分基本上都是用FICO来做的。各家平台会尝试着用机器学习、神经网络等大数据处理方法。
大数据消费金融：消费金融对大数据的依赖是天然形成的。比如说消费贷、工薪贷、学生贷，这些消费型的金融贷款很依赖对用户的了解。所以必须对用户画像进行分析提炼，通过相关模型展开风险评估，并根据模型及数据从多维度为用户描绘一个立体化的画像。

（Volume），即数据巨大，从TB级别跃升到PB级别；第二，多样性（Variety），即数据类型繁多，不仅包括传统的格式化数据，还包括来自互联网的网络日志、视频、图片、地理位置信息等；第三，速度（Velocity），即处理速度快；第四，真实性（ Veracity ），即追求高质量的数据。

除了之前提到的机型、操作系统、分辨率、ip、网络模式等基础数据的收集，跟操作严格绑定的有页面编号、内容id、用户id、操作类型、控件id等相关数据，共34个属性。
在营销领域，RFM模型是用来衡量客户的价值和客户的创利能力的重要工具和手段 R(Recency)，最近一次消费，指上一次购买的时间到现在的距离。
 F(Frequency)，消费频率，即用户在限定的期间内产生购买的总次数。产生购买越频繁的用户，忠诚度越高。
 M(Monetary)，某个用户所有消费金额的平均值。这一指标也可以验证“帕雷托法则”(Pareto’s Law)，粗略来说，公司80%的收入来自20%的顾客。
 
 完整的团队，包括前端工程师埋点采集数据，架构师架构数据，商业分析师提供各种分析，还有数据科学家进行建模，基础工程师。
 
 大家看到中国从免费的APP数据分析时代( Talking Data、友盟、易观)，逐步发展到收费产品分析时代(比如GrowingIO、神策数据、诸葛IO)。
 

还可以找到很多数据应用场景，例如休眠客户唤醒，分期客户寻找，高净值客户寻找，流失客户挽留，高频交易客户激活、关联产品推荐、理财产品定位、客户分群营销等。一般非精准营销的短信转化率在千分之二左右，但是精准营销的短信转化率在百分之二到百分之五，有的可以达到百分之六。客户分群的精准营销短信，其转化率为非精准营销的十倍以上，成本为十分之一，营销周期可以为缩短十倍。
客户在外部的行为数据，可以分为搜索数据，点击浏览数据，位置数据，社交数据。其中搜索数据代表人的内心需要，我们叫做intention数据。社交数据代表人的观点，我们叫做comments数据，点击浏览数据是interest数据，代表人的喜好和兴趣。位置数据比较特殊，记录了人的线下行为轨迹，代表一个人在社会的角色，可以认为是社会角色role数据
数学模型揭示了海量数据背后相似人群的特征，同时为金融企业的精准营销打开了一扇大门。仅仅依靠数据标签和用户画像无法直接帮助金融企业识别出客户的需求，无法帮助金融实现业务的提升和产品销售的提升。

传统的FICO评分模型的基本思想是比较借款人信用历史资料与数据库中的全体借款人的信用习惯，检查借款人的发展趋势跟经常违约、随意透支，甚至申请破产等各种陷入财务困境的借款人的发展趋势是否相似

大数据的应用要注意个人的隐私保护。ZestFinance在利用个人消费者的大数据进行信用评估时，很多数据会涉及个人隐私，如对于个人社交网络的数据（微信朋友圈）和电商交易的数据、通话记录、微博的数据等应用，美国对个人隐私的保护是有明确的边界的。而国内关于个人隐私方面的保护目前处于空白，已经出现国内一些互联网金融公司为了进行信用评估，忽视个人消费者的知情权和隐私保护。因此在利用大数据进行信用评估的时候，要考虑使用个人隐私的合规性前提。

我们定义用户和用户之间如果共用某些核心信息那么他们之间就存在紧密联系，这些核心信息可以是手机设备、电话号码、身份证、银行卡号、邮箱等

金融客户拥有较为丰富的个人属性数据、资产数据、信用数据、交易数据。缺少客户在本金融企业之外的金融数据和个人行为数据。
市场上最好的数据是运营商数据和银联数据，
目前电信的DPI标签集中在客户固网访问行为，也就是在PC上的浏览标签，联通的DPI标签集中在移动互联网的访问行为行为和标签，中国移动的DPI标签还在挖掘开发中。移动、电信、联通覆盖的移动互联网用户比例分别为6:2:2，中国移动占了大部分，客户质量较高。另外可以提供移动互联网访问行为表标签的数据厂商是TakingData、极推、个推等第三方数据服务商。
银联的数据集中在刷卡的消费和支出的分级信息，以卡、POS为单位，可以用于风控和信用评估，具体个人的刷卡信息不能提供。

数据在金融企业的应用很曲折，数据部门同业务部门在数据应用效果和场景应用需要长时间磨合。有的保险企业数据部门即使将整理好的潜在客户名单发给业务部门，业务部门也不相信，也不会打电话去尝试。
活跃客户没有明确的定义，一般以月度发生过一次交易/查询以上的客户定义为活跃客户。金融企业的僵尸客户，可以定义为是一年业务之内没有同金融企业发生过任何交易的客户，一般在30%左右

金融企业具有典型的帕累托效应，就是20%甚至10%的客户拥有80%以上的资产和交易额，这些客户为金融企业贡献了较大的收入和利润。另外潜在的高价值客户比例接近或超过已有的高价值客户;休眠客户中至少30%可以转化为活跃的客户;已有客户中，潜在的金融需求，金融企业只能了解其中的30%。因此第一方数据的分析和应用是金融行业数据应用的首要方向。
我们可以分析一群客户，例如这些客户在2015年人均购买理财产品为50万，但是2016年人均购买理财产品低于一万。我们可以定义其为流失的高价值客户，银行可以为这些客户定制一些理财产品，利用短信向这些客户推荐定制的产品

央行的征信系统，所覆盖的人群还是非常有限，远远低于美国征信体系对人口的85%的覆盖。目前我国个人有征信记录的仅有约3.2亿人，约占13.5亿人口中的23.7%

用户画像，即用户信息标签化，就是企业通过收集与分析消费者社会属性、生活习惯、消费行为等主要信息的数据之后，描绘出一个真实用户的虚拟物。
用户画像的焦点工作就是为用户打“标签”，而一个标签通常是人为规定的高度精炼的特征标识，如年龄、性别、地域、用户偏好等，最后将用户的所有标签综合来看，就可以勾勒出该用户的立体“画像”了。
该平台需要回答的核心问题是：用户是谁? 用户需求是什么? 用户在哪里?
构建用户画像平台所需的数据分成用户、商品、渠道三类实体。
1、用户：数据维度包括自然特征、兴趣特征、社会特征、消费特征。从数据特点上看，又可分为基本属性和衍生标签，基本属性包括年龄、性别、地域、收入等客观事实数据，衍生标签属于基本属性为依据，通过模型规则生成的附加判断数据。
2、商品：数据维度包括商品定位和商品属性。商品属性即商品的功能、颜色、能耗、价格等事实数据，商品定位即商品的风格和定位人群，需要和用户标签进行匹配。
3、渠道：渠道分为信息渠道和购买渠道。用户在信息渠道上获得资讯，在购买渠道上进行商品采购。不同类型的用户对渠道有不同的偏好，精准的选择对应的渠道才能提高效率和收益
1、用户数据维度分解：
自然特征：性别，年龄，地域，教育水平，出生日期，职业，星座。
兴趣特征：兴趣爱好，使用APP/网站,浏览/收藏内容，互动内容，品牌偏好，产品偏好。
社会特征:婚姻状况，家庭情况，社交/信息渠道偏好。
消费特征：收入状况,购买力水平，已购商品，购买渠道偏好，最后购买时间，购买频次。
针对不同角色人员的需求(如市场、销售、研发等)，设计各角色人员在用户画像工具中的使用功能和应用/操作流程。下面以两个应用场景为例，让大家明白如何利用用户画像。
场景一，按需设计：改变原有的先设计、再销售的传统模式，在研发新产品前，先基于产品期望定位，在用户画像平台中分析该用户群体的偏好，有针对性的设计产品，从而改变原先新产品高失败率的窘境，增强销售表现。比如，某公司想研发一款智能手表，面向28-35岁的年轻男性，通过在平台中进行分析，发现材质=“金属”、风格=“硬朗”、颜色=“黑色”/”深灰色”、价格区间=“中等”的偏好比重最大，那么就给新产品的设计提供了非常客观有效的决策依据。
场景二，精准营销：针对已有产品，寻找所偏好的精准人群分类，以及这些人群在信息渠道和购买渠道上的分布比例，来决定广告投放和活动开展的位置、内容等，实现精准营销。

第一步：弄清目标和当前的主要问题
在开始之前，确立产品的目标，弄清当前最亟待解决的问题是至为重要的事。
比如，一个电商类产品已经确立其目标是提升销售额。接来下，就要分析当前最主要的问题是什么?是新用户的增长不够多，还是老用户的重复购买率太低?这些问题，可以很方便的通过诸葛io分析得到。
第二步：找出问题相关的数据指标
弄清目标和主要问题后，下一步是要找出和问题最直接相关的数据指标。
比如，如果当前的问题是用户的重复购买率低，那么还进一步分析：用户在第一次购买多久之后的购买率会有显著的降低?哪些人群的重复购买率明显的低于或高于全部人群的平均值?
总结起来就是：要尽可能精准的定位问题的点(时间、人群、渠道……)。
第三步：对问题指标的相关人群进行画像分析，探究问题背后的可能原因
找到较为精确的问题点及相关指标后，可以围绕这些指标做背后人群的画像分析，看能不能找到潜在的原因。
比如，分析重复购买率明显高于均值的用户的群体画像，将其人群属性、行为特点与其他用户做对比，找到不同点，分析这些不同点与重复购买率之间的关系(需要的话，可以直接或间接联系少量的用户以做验证)。
第四步：改进产品或运营
在上一步，您已经分析出了一项或几项可能影响用户增长的原因。接下来，您需要做的是从可能性以及改进成本等方面评估，并对产品或运营做出改进。
比如，改进易用性差的功能，或针对有问题地区的用户增加引导。
第五步：观察指标和画像，分析改进效果
改进后，对问题指标及问题相关人群进行持续的观测，验证是否达到了预期的效果。如果达到了预期的效果，则继续按照上面的步骤分析新的问题并加以解决。如果未达到预期的效果，也可以继续按照上面的步骤继续分析问题的原因，或者放弃转向其他问题。


1.要全体数据、不要样本
2.要效率、不要精确性
3.要相关关系、不要因果关系

通过用户群体画像，我们已经能够持续的监测产品运营状况，比如：观察产品关键指标的变化、关注用户到目标的转化趋势、分析用户的留存回访……

留存的概念一点也不陌生，常见的统计周期有次日、7日、14日、30日等。通常又分为新用户留存和活跃用户留存。

国内现有大量数据服务商提供app数据监测，如TalkingData、易观、Questmobile等

什么是数据库扩展的version + ext方案？
使用ext来承载不同业务需求的个性化属性，使用version来标识ext里各个字段的含义
而帖子库ext字段里json的key，统一由数字来表示，减少存储空间。
从一早就确定了“外置索引，统一检索服务”的技术路线：
（1）数据库提供“帖子id”的正排查询需求
（2）所有非“帖子id”的个性化检索需求，统一走外置索引




成功的大数据规划聚焦于四个核心要素：应用场景、数据产品、分析模型和数据资产
在合理的大数据组织架构下，有两类数据团队，一类是各事业部中的数据团队;第二类是中央数据部门的数据团队
而中央数据部门的数据能力要求较为复杂，包括六大方面的能力，即数据分析、用户研究、数据产品、算法工程、数据统计和数据平台

数据报告分为两种。一种是追踪型的数据报告，或者成为dashboard。它是对日常业务数据高频率的展现，关键在于发现问题，而不是解决问题。它一般用于回答“怎么了”。这类报告往往是规律性地长期进行制作。另一种就是解决问题的数据报告，它一般是专题型的研究报告，用于回答“为什么”。这类报告往往是不定期地进行制

数据分析的方法论很多，这里我给大家介绍一些常见的框架。

AARRR：增长黑客的海盗法则，精益创业的重要框架，从获取(Acquisition)、激活(Activation)、留存(Retention)、变现(Revenue)和推荐(Referral)5个环节增长。
4P理论：经典营销理论，认为产品(Product)、价格(Price)、渠道(Place)和促销(Promote)是影响市场的重要因素。
5W2H分析法：从Why、When、Where、What、Who、How、How much 7个常见的维度分析问题。
PEST分析法：从政治(Politics)、经济(Economy)、社会(Society)、技术(Technology)四个方面分析内外环境，适用于宏观分析。
SWOT分析法：从优势(Strength)、劣势(Weakness)、机遇(Opportunity)、威胁(Threat)四个方面分析内外环境，适用于宏观分析。

观察用户在你产品内的行为路径是一种非常直观的分析方法。在用户分群的基础上，一般抽取3-5个用户进行细查，即可覆盖分群用户大部分行为规律。
漏斗是用于衡量转化效率的工具，因为从开始到结束的模型类似一个漏斗，因而得名。
漏斗分析要注意的两个要点：第一，不但要看总体的转化率，还要关注转化过程每一步的转化率;第二，漏斗分析也需要进行多维度拆解，拆解之后可能会发现不同维度下的转化率也有很大差异。
某企业的注册流程采用邮箱方式，注册转化率一直很低，才27%;通过漏斗分析发现，主要流失在【提交验证码】的环节。

试想一下如果没有ABtest，那新项目上线后的收益如何排除季节因素、市场环境因素的影响，而且一个页面上如果同时做多处改动，如何评判是哪个改动造成的收益或损失？这对一个理性思维的人是不可接受的。

我们大致可以把一个产品的运营分为用户获取和用户拓展两个阶段。在用户获取阶段，用户因为自然原因或一些营销事件（例如广告、社交媒体传播）产生对外卖的注意，进而产生
了兴趣，并在合适的时机下完成首购，从而成为外卖新客。在这一阶段，运营的重点是提高效率，通过一些个性化的营销和广告手段，吸引到真正有潜在需求的用户，并刺激其转化。在用户完成转化后，接下来的运营重点是拓展用户价值。这里有两个问题：第一是提升用户价值，具体而言就是提升用户的单均价和消费频次，从而提升用户的LTV（life-time value)。基本手段包括交叉销售（新品类的推荐）、向上销售（优质高价供给的推荐）以及重复购买（优惠、红包刺激重复下单以及优质供给的推荐带来下单频次的提升）；第二个问题是用户的留存，通过提升用户总体体验以及在用户有流失倾向时通过促销和优惠将用户留在外卖平台。
除去提供常规的用户基础属性（例如年龄、性别、职业、婚育状况等）以及用户偏好之外，还需要考虑这么几个问题：1）什么样的用户会成为外卖平台的顾客（新客识别）；2）用户所处生命周期的判断，用户是否可能从平台流失（流失预警）；3）用户处于什么样的消费场景（场景识别）
新客的潜在转化概率，受到新客的人口属性（职业、年龄等）、所处地域（需求的因素）、周围人群（同样反映需求）以及是否有充足供给等因素的影响；而对于新客的偏好和消费力，从新客在到店场景下的消费行为可以做出推测。另外用户的工作和居住地点也能反映他的消费能力

获取一个新顾客的成本是维系现有顾客成本的5倍！
如果将顾客流失率降低5%，公司利润将增加25%~85%
用户流失的原因通常包括：竞对的吸引；体验问题；需求变化。我们借助机器学习的方法，构建用户的描述特征，并借助这些特征来预测用户未来流失的概率。这里有两种做法: 第一种是预测用户未来若干天是否会下单这一事件发生的概率。这是典型的概率回归问题，可以选择逻辑回归、决策树等算法拟合给定观测下事件发生的概率；第二种是借助于生存模型，例如COX-PH模型，做流失的风险预测。
1）数据多样性，存在大量非结构化数据例如用户地址、菜品名称等。需要用到自然语言处理技术，同时结合其他数据进行分析。